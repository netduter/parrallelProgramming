# Εργασία στο μάθημα παράλληλοι υπολογιστές

**Νικόλαος Καπονικολός (21628)**

**Ιωάννης Σομός (21685)**

## Γενικά σχόλια - παρατηρήσεις

1. Τα προγράμματα που αφορούν την εξέταση είναι υλοποιημένα στον φάκελο `src`.
   O φάκελος `experiments` περιλαμβάνει "πειράματα" τα οποία έγιναν
   στα πλαίσια διερεύνησης του παράλληλου προγραμματισμού.
2. Τα προγράμματα σε C++ απαιτούν το `std=c++11` flag για να γίνουν compile.
3. Όλα τα προγράμματα δέχονται σαν είσοδο δυο παραμέτρους
   που αντιστοιχούν στο αρχείο εισόδου και στο αρχείο εξόδου.

Ενδεικτικό compile και εκτέλεση:

```sh
$ mkdir -p build
$ g++ -std=c++11 experiments/exercise1-cpp/*.cpp -o build/main
$ build/main data/100.fastq build/results.txt
```

## Άσκηση 1

Η άσκηση 1 έχει υλοποιηθεί σε C (`src/main.c`) και σε C++ (`experiments/exercise1-cpp`).

Το πρόγραμμα στη C++ χρησιμοποιεί regular expressions για να αναζητήσει τις ακολουθίες DNA.

Είναι 9 φορές πιο αργό από το πρόγραμμα στη C όταν η είσοδος είναι 100,000 ακολουθίες.

Η καθυστέρηση αυτή πιθανότατα να οφείλεται στα regular expressions.

## Άσκηση 2

Η άσκηση 2 έχει υλοποιηθεί σε C (`src/mpi.c`) και σε C++
(`experiments/exercise2-cpp/middle-to-middle` & `experiments/exercise2-cpp/middle-to-end`).

Η πρώτη υλοποίηση περιλαμβάνει τα εξής βήματα:

1. Προεπεξεργασία του αρχείου για τον υπολογισμό των ακολουθιών που θα επεξεργαστούν.
2. Δημιουργία ενός μεγάλου μηνύματος που περιέχει όλες τις ακολουθίες
   και αποστολή του με `MPI_Bcast` σε όλους τους workers.
3. Λήψη με `MPI_Recv` των αποτελεσμάτων και αποθήκευση στο αρχείο.

Η παραπάνω υλοποίηση είναι η ταχύτερη που καταφέραμε στο MPI
και χρησιμοποιεί τον ελάχιστο αριθμό αποστολής και λήψεις μηνυμάτων.

Οι δυο επόμενες υλοποιήσεις σε C++ κάνουν broadcast ένα
μικρότερο μήνυμα και στη συνέχεια λαμβάνουν τα αποτελέσματα.

Η διαδικασία επαναλαμβάνεται πολλές φορές μέχρι να διαβαστεί και επεξεργαστεί το αρχείο.

Εδώ προκύπτει μια ενδιαφέρουσα παρατήρηση. Στην περίπτωση που ο head node
κάνει broadcast και αμέσως μετά receive για να λάβει τα αποτελέσματα
(`experiments/exercise2-cpp/middle-to-middle`) το πρόγραμμα εκτελείται
4 φορές πιο αργά από το πρόγραμμα που κάνει όλα τα broadcast μαζί και
τα receive στο τέλος (`experiments/exercise2-cpp/middle-to-end`).

Μια λογική ερμηνεία αυτής της διαφοράς είναι ότι όταν ο head node κάνει receive
τα αποτελέσματα των worker nodes δεν έχουν αποσταλεί και έτσι υπάρχει αναμονή.

Στην περίπτωση που τα receive γίνονται μαζικά στο
τέλος πάντα υπάρχει μήνυμα πληροφορίας από τους workers.

Ενδεχόμενο πρόβλημα που μπορεί να παρουσιαστεί στις υλοποιήσεις
`src/mpi.c` και `experiments/exercise2-cpp/middle-to-end` είναι
να [συμβεί υπερχείλιση](https://stackoverflow.com/questions/29776261/how-does-mpi-send-work-when-the-application-buffer-size-is-greater-than-mpi-buff) στους εσωτερικούς buffers του MPI.

## Άσκηση 3

Η άσκηση 2 έχει υλοποιηθεί σε C (`src/omp.c`) και σε C++ (`experiments/exercise3-cpp`).

Αρχικά κάνουμε μια προεπεξεργασία του αρχείου για να δούμε πόσες ακολουθίες περιέχει.

Στο κομμάτι που εκτελείται παράλληλα κάθε thread ανοίγει ένα file link με το αρχείο
και διαβάζει το τμήμα που του αντιστοιχεί. (Υπολογίζεται με βάση τον αριθμό του thread).

Τα αποτελέσματα της επεξεργασίας αποθηκεύονται σε ξεχωριστά προσωρινά αρχεία. Η ονοματοδοσία
τους είναι τέτοια ώστε να αναγνωρίζεται εύκολα ποιο thread έχει γράψει στο καθένα.

Μόλις τελειώσει το παράλληλο τμήμα του προγράμματος τα
προσωρινά αρχεία συνενώνονται σε ένα τελικό και διαγράφονται.

Σε αυτήν την υλοποίηση δεν υπάρχει κρίσιμο τμήμα.
Ωστόσο ενδέχεται να υπάρχει ανταγωνισμός
για το ποιος θα διαβάσει από το αρχείο.

Στις βάσεις δεδομένων γνωρίζουμε ότι υπάρχει το read lock,
έτσι ώστε να υπάρχει συνέπεια της εικόνας που έχουν τα δεδομένα.

Εξαρτάται από το λειτουργικό αν θα υπάρξουν συνθήκες ανταγωνισμού.
Στα Linux επιτρέπεται προγράμματα να προσπελαύνουν το ίδιο αρχείο ταυτόχρονα.

Το πρόγραμμα δέχεται 3 ορίσματα από την γραμμή εντολών. Το πρώτο είναι το
αρχείο εισόδου, το δεύτερο το αρχείο εξόδου, και το τρίτο ο αριθμός των threads.

```sh
$ build/main data/100.fastq build/results.txt
```

## Ερώτημα 4

### Χρόνοι εκτέλεσης

<table style="width:100%">
  <tr>
    <th></th>
    <th colspan="3">Είσοδος προγράμματος</th>
  </tr>
  <tr>
    <th>Πρόγραμμα</th>
    <th>100</th>
    <th>10K</th>
    <th>100K</th>
  </tr>
  <tr>
    <td>Σειριακό C</td>
    <td>0.010s</td>
    <td>0.343s</td>
    <td>3.143s</td>
  </tr>
  <tr>
    <td>Σειριακό C++</td>
    <td>0.070s</td>
    <td>1.908s</td>
    <td>17.183s</td>
  </tr>
  <tr>
    <td>MPI C broadcast</td>
    <td>2.048s</td>
    <td>2.708s</td>
    <td>6.422s</td>
  </tr>
  <tr>
    <td>MPI C++ broadcast/receive μαζί</td>
    <td>2.590s</td>
    <td>14.269s</td>
    <td>150s</td>
  </tr>
  <tr>
    <td>MPI C++ broadcast αρχικά/receive μαζικά στο τέλος</td>
    <td>2.589s</td>
    <td>6.513s</td>
    <td>41.349s</td>
  </tr>
  <tr>
    <td>OpenMP C </td>
    <td>0.019s</td>
    <td>0.348s</td>
    <td>2.958s</td>
  </tr>
  <tr>
    <td>OpenMP C++ </td>
    <td>0.022s</td>
    <td>0.492s</td>
    <td>2.949s</td>
  </tr>
</table>

### Throughput

<table style="width:100%">
  <tr>
    <th></th>
    <th colspan="3">Είσοδος προγράμματος</th>
  </tr>
  <tr>
    <th>Πρόγραμμα</th>
    <td>100</td>
    <td>10,000</td>
    <td>100,000</td>
  </tr>
  <tr>
    <td>Σειριακό C</td>
    <td>10,000</td>
    <td>29,155</td>
    <td>31,817</td>
  </tr>
  <tr>
    <td>Σειριακό C++</td>
    <td>1,428</td>
    <td>5,241</td>
    <td>5,820</td>
  </tr>
  <tr>
    <td>MPI C broadcast</td>
    <td>49</td>
    <td>3,563</td>
    <td>15,571</td>
  </tr>
  <tr>
    <td>MPI C++ broadcast/receive μαζί</td>
    <td>39</td>
    <td>701</td>
    <td>667</td>
  </tr>
  <tr>
    <td>MPI C++ broadcast αρχικά/receive μαζικά στο τέλος</td>
    <td>39</td>
    <td>1,535</td>
    <td>2,418</td>
  </tr>
  <tr>
    <td>OpenMP C</td>
    <td>5,263</td>
    <td>28,735</td>
    <td>33,806</td>
  </tr>
  <tr>
    <td>OpenMP C++</td>
    <td>4,545</td>
    <td>20,325</td>
    <td>33,909</td>
  </tr>
</table>

### Επιτάχυνση

<table style="width:100%">
  <tr>
    <th></th>
    <th colspan="3">Είσοδος προγράμματος</th>
  </tr>
  <tr>
    <th>Πρόγραμμα</th>
    <td>100</td>
    <td>10,000</td>
    <td>100,000</td>
  </tr>
  <tr>
    <td>Σειριακό C</td>
    <td>1</td>
    <td>1</td>
    <td>1</td>
  </tr>
  <tr>
    <td>MPI C broadcast</td>
    <td>0.006</td>
    <td>0.13</td>
    <td>0.49</td>
  </tr>
  <tr>
    <td>MPI C++ broadcast/receive μαζί</td>
    <td>0.004</td>
    <td>0.024</td>
    <td>0.021</td>
  </tr>
  <tr>
    <td>MPI C++ broadcast αρχικά/receive μαζικά στο τέλος</td>
    <td>0.004</td>
    <td>0.05</td>
    <td>0.08</td>
  </tr>
  <tr>
    <td>OpenMP C</td>
    <td>0.53</td>
    <td>0.99</td>
    <td>1.06</td>
  </tr>
  <tr>
    <td>OpenMP C++</td>
    <td>0.45</td>
    <td>0.70</td>
    <td>1.07</td>
  </tr>
</table>

*Σημείωση: Στα προγράμματα MPI έχουν χρησιμοποιηθεί
10 processes και στου OpenMP 10 threads.*

### Σχόλια & συμπεράσματα

1. Στη συγκεκριμένη εκτέλεση έχουμε επιτάχυνση μόνο με
   το OpenMP όταν η είσοδος είναι 100,000 ακολουθίες.
2. Θα μπορούσαμε ίσως να δούμε επιτάχυνση στο MPI αν το υπολογιστικό
   κόστος ήταν σημαντικά μεγαλύτερο από το κόστος επικοινωνίας.
3. Στο MPI αλλά και στο OpenMP αν αυξήσουμε τον αριθμό των διεργασιών ή νημάτων
   αντίστοιχα τότε παρατηρούμε επιβράδυνση του προγράμματος. Η εύρεση του
   βέλτιστου αριθμού threads και processes είναι μια δύσκολη διαδικασία.
4. Οι γλώσσες ανώτερου επιπέδου προσφέρουν προγραμματιστικές ευκολίες
   οι οποίες αν καταχραστούν οδηγούν σε προγραμματιστική φλυαρία.
   Συχνά οι ευκολίες συνοδεύονται από μείωση της απόδοσης.
5. Το debugging στον παράλληλο προγραμματισμό είναι εξαιρετικά επίπονο.
   Στη C++ η δυνατότητα για exceptions, οδηγεί πιο εύκολα στην
   ανακάλυψη των σφαλμάτων. Ο μηχανισμός των exceptions έχει κόστος
   το οποίο θα πρέπει να αξιολογείται σε μια εφαρμογή απόδοσης.
